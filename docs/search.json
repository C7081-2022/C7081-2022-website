[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to C7081. This module is a survey of machine learning and statistical methods including supervised and unsupervised classification, regression, and tree-based methods. There is an emphasis on practical applications using a series of data stories and lab exercises, along with lectures on selected topics and readings. A prerequisite is a basic working knowledge of R programming and introductory statistics."
  },
  {
    "objectID": "index.html#c7081-resources",
    "href": "index.html#c7081-resources",
    "title": "Home",
    "section": "C7081 Resources",
    "text": "C7081 Resources\n\nTextbook: James et al. 2021 Introduction to statistical learning 2ed\nSlack workspace (for enrolled students)\nOffice hours (Slack, Fridays by appointment)"
  },
  {
    "objectID": "index.html#harper-adams-data-science",
    "href": "index.html#harper-adams-data-science",
    "title": "Home",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01.html#c7081-resources",
    "href": "lab01.html#c7081-resources",
    "title": "Lab 01 linear alg.",
    "section": "C7081 Resources",
    "text": "C7081 Resources"
  },
  {
    "objectID": "lab01.html#harper-adams-data-science",
    "href": "lab01.html#harper-adams-data-science",
    "title": "Lab 01 linear alg.",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase data science skills (e.g. a github project website that could be part of your cv). For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\n\n\n\nDay\nTopics\nLabs\nReadings\n\n\n\n\nMonday\n00 Module overview\n01 Introduction\n02 Statistical learning\nLab guidance\nLab 01 Linear algebra fun\nLab 02 R programming refresh\nJames et al. 2021 Ch 1,2\nEfron 2020\n\n\nTuesday\n03 Linear regression\n04 Classification\nLab 03 Linear regression\nLab 04 Classification\nJames et al. 2021 Ch 3,4\nMelesse 2018\n\n\nWednesday\n05 Bootstrapping\n06 Model selection\nLab 05 Resampling\nLab 06 Model selection\nJames et al. 2021 Ch 5,6\nAho 2014\n\n\nThursday\n07 Non-linear models\n08 Decision trees\nLab 07 Non-linear models\nLab 08 Decision trees\nJames et al. 2021 Ch 7,8\nBarnard 2019\nOtukei 2010\n\n\nFriday\n09 Support vector machines\n10 Unsupervised learning\nLab 09 SVM\nLab 10 Unsupervised learning\nJames et al. 2021 Ch 9,12\nEbrahimi 2017\nHowell 2020"
  },
  {
    "objectID": "schedule.html#references",
    "href": "schedule.html#references",
    "title": "Schedule",
    "section": "References",
    "text": "References\nTextbook: James et al. 2021 Introduction to statistical learning 2ed\nall refs zip\nAho, K., Derryberry, D., Peterson, T., 2014. Model selection for ecologists: the worldviews of AIC and BIC. Ecology 95, 631–636.\nBarnard, D.M., Germino, M.J., Pilliod, D.S., Arkle, R.S., Applestein, C., Davidson, B.E., Fisk, M.R., 2019. Cannot see the random forest for the decision trees: selecting predictive models for restoration ecology. Restoration Ecology 27, 1053–1063.\nEbrahimi, M.A., Khoshtaghaza, M.H., Minaei, S., Jamshidi, B., 2017. Vision-based pest detection based on SVM classification method. Computers and Electronics in Agriculture 137, 52–58.\nEfron, B., 2020. Prediction, Estimation, and Attribution. Journal of the American Statistical Association 115, 636–655.\nHowell, O., Wenping, C., Marsland, R., Mehta, P., 2020. Machine learning as ecology. J. Phys. A: Math. Theor. 53, 334001.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2021. An Introduction to Statistical Learning: with Applications in R, Springer Texts in Statistics 2ed. Springer-Verlag, New York.\nMelesse, S., Sobratee, N., Workneh, T., 2016. Application of logistic regression statistical technique to evaluate tomato quality subjected to different pre- and post-harvest treatments. Biological Agriculture & Horticulture 32, 277–287.\nOtukei, J.R., Blaschke, T., 2010. Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms. International Journal of Applied Earth Observation and Geoinformation, Supplement Issue on “Remote Sensing for Africa – A Special Collection from the African Association for Remote Sensing of the Environment (AARSE)” 12, S27–S31."
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab00-guidance.html",
    "href": "lab00-guidance.html",
    "title": "Lab 00 Guidance",
    "section": "",
    "text": "There are several recommendations you should follow to get the most out of these exercises and this module as a whole.\n\nSet up a separate script for each lab (e.g. an R script) to organize and document your work\nAnswer each question formally and fully\nUse a fully commented, reproducible script style with a Header, Contents, and clickable Section blocks. This will help you practice Best Practice (see what I did there?), and will make the script useful to yourself in the future as a reference (e.g., the next time you want to set up a K means analysis script…)\nType 100% of your own code: do not copy and paste anything (practice, practice + troubleshooting error messages for typos and other issues)\nConsider writing your lab scripts using R Markdown\nConsider setting up a Github repository for your lab scripts"
  },
  {
    "objectID": "lab00-guidance.html#harper-adams-data-science",
    "href": "lab00-guidance.html#harper-adams-data-science",
    "title": "Lab 00 Guidance",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01-lin-alg.html",
    "href": "lab01-lin-alg.html",
    "title": "Lab 01 Linear alg.",
    "section": "",
    "text": "Linear algebra is the (math) foundation of statistics and data science. While it is possible to practice data analysis without a robust knowledge of algebra, a little bit helps. The purpose here is to highlight and review the key linear algebra concepts and to demonstrate a few examples. By the end of this lab you should be able to:\n\nDescribe the structure of vectors and matrices\nPerform math functions with linear algebra structures\nDemonstrate awareness of linear algebra utility"
  },
  {
    "objectID": "lab01-lin-alg.html#vectors",
    "href": "lab01-lin-alg.html#vectors",
    "title": "Lab 01 Linear alg.",
    "section": "2 Vectors",
    "text": "2 Vectors\n\n2.1 The basic vector concept\nVectors can be conceptualized as a list of numerical values (elements) that may be arranged in columns or rows. A formal difference between column and row vectors is the notation for their arrangement, where a vector has n elements, a row vector is a matrix with \\([1 \\times n]\\) elements; a column vector has \\([n \\times 1]\\) elements.\nColumn vector:\n\\(a=\\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\)\nRow vector:\n\\(b=\\begin{bmatrix} 2, 1, 3 \\end{bmatrix}\\)\nVectors have a specific order such that:\n\\((2,1,3) \\neq (1,2,3)\\)\nWe can generalize the notation for a vector containing n elements as an n-vector such that:\n\\(a=\\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix}\\)\nwhere each element \\(a_i\\) is a numerical value and the vector can be written as \\(a=(a_1,\\dots,a_n)\\).\nWe can represent vectors graphically, e.g. here is an example in R graphing 2 2-vectors.There are some conventions in geometry and math notation here, that are not necessarily the same as the way we store data structures in a programming language…\n\n# imagine two vectors that each contain the x,y coordinates of a point\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\n\n\n\n\n\n\nVectors in R are always printed in the ‘row format’, regardless of math notation.\n\na <- c(4,2,3)\na\n\n[1] 4 2 3\n\n\n\n\n2.2 Transposing\nTansposing a vector is when a column or row vector is turned into the opposite orientation. The transpose is notated with the symbol \\(\\intercal\\)\nColumn to row format\n\\(\\begin{bmatrix} 4 \\\\ 8 \\\\ 5 \\end{bmatrix} ^ \\intercal = [4, 8, 5]\\)\nRow to column format\n\\([4, 8, 5] ^ \\intercal = \\begin{bmatrix} 4 \\\\ 8 \\\\ 5 \\end{bmatrix}\\)\n\n# transpose in R\na <- c(5,7,6)\n\n# the t() function forces the object as a matrix\nt(a)\n\n     [,1] [,2] [,3]\n[1,]    5    7    6\n\n# multiple transpose, just to demonstrate\nt(t(a))\n\n     [,1]\n[1,]    5\n[2,]    7\n[3,]    6\n\n\n\n\n2.3 Multiplication\nA number \\(b\\) and a vector \\(a\\) can be multiplied together\n\\(b \\times a =\\begin{bmatrix} b \\times a_1 \\\\ b \\times a_2 \\\\ \\vdots \\\\ b \\times a_n \\end{bmatrix}\\)\nThus\n\\(5 \\times \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}\\)\n\n# vector multiplication in R\na <- c(2,4,5)\nb <- 3\n\na*b\n\n[1]  6 12 15\n\n\nGraphing vector multiplication\n\n# our 2 vectors from before\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\n\n\n\n\n\n\n\n\n2.4 Addition\nLet \\(a\\) and \\(b\\) be n-vectors, where \\(a\\) and \\(b\\) are of the same dimensions.\n\\(a + b = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} = \\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix} = b+a\\)\nWith numbers\n\\(a + b = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 2 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 5 \\\\ 13 \\end{bmatrix}\\)\n\n# vector addition in R\na <- c(3, 5, 1)\nb <- c(14, 3, 5)\n\na + b\n\n[1] 17  8  6\n\n\n\n# our 2 vectors from before\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\nvec3 <- vec1 + vec2\n\n\n\n\n\n\n\n\n2.5 Vector inner product\nThe inner product of a vector is obtained by multiplying two vectors and summing the result (NB this is sometimes called the dot product).\n\\(\\sum a*b = a \\cdot b = a_1b_1 + \\dots + a_nb_n\\)\nwith numbers\n\\(\\sum (\\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix} \\times \\begin{bmatrix} 4 \\\\ 2 \\\\ 8 \\end{bmatrix}) = \\sum \\begin{bmatrix} 4 \\\\ 6 \\\\ 40 \\end{bmatrix} == 50\\)\n\n# dot product in R\na <- c(1,3,5)\nb <- c(4,2,8)\n\nsum(a * b)\n\n[1] 50\n\n\n\n# alternative syntax for the dot product\na %*% b\n\n     [,1]\n[1,]   50\n\n\n\n\n2.6 Magnitude (aka the “norm”) of a vector\nThere are several ways to measure the “bigness” of a vector, sometimes called the norms. Although we will not go into detail here, there are two types of norm to be aware of. These may seem a little esoteric for our purposes here, but they are used “under the bonnet” for many statistical and machine learning calculations (thus, you may encounter them and should probably be aware of them).\nL1 norm (aka the outer norm) - this is the overall absolute magnitude of vector values\nL2 norm (aka the inner norm) this is the linear (“Euclidean”) distance of the vector from the origin (the zero value in n-dimensional space).\n\nL1 norm\nThe L1 norm is calculated by summing the absolute value of all vector elements.\nTake a vector \\(a = (2, -4, 5)\\)\n\\(||a||_1 = \\sum(|a_1|+ \\dots + |a_n|)\\)\n\\(||a||_1 = (2 + 4 + 5) = 11\\)\n\n\nL2 norm\nThe L2 norm is calculated by taking the square root of the summed values of the squared values of each element of a vector.\nTake a vector \\(b = (-1, 0, 3)\\)\n\\(||b||_2 = \\sqrt(b_1^2+ \\dots + b_n^2)\\)\n\\(||b||_2 = \\sqrt(1 + 0 + 9) = 3.16\\)\n\n# norms in R\na <- c(2, -4, 5)\nb <- c(-1, 0, 3)\n\nsum(abs(a)) # L1\n\n[1] 11\n\nsqrt(sum(b^2)) # L2\n\n[1] 3.162278\n\n# alternative calculation using the norm() function\n# ?norm\n\nnorm(matrix(a), type = \"O\") # L1\n\n[1] 11\n\nnorm(matrix(b), type = \"2\") # L2\n\n[1] 3.162278\n\n\n\n\n\n2.7 Special vectors\nThere are a few special cases of vectors we may encounter (but which are certainly there “doing work” for us), like the 0-vector and the 1-vector. These are simply vectors where all values assume either zero or one, respectively. These are often used in linear models to encode data for matrix calculations (but we will leave it at that for now).\n\n# 0-matrix, n=10\nrep(0, 10)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n# 1-matrix, n=8\nrep(1, 8)\n\n[1] 1 1 1 1 1 1 1 1\n\n\n\n\n2.8 Orthogonal vectors\nOrthogonal vectors are used in a number of statistical methods, e.g. multivariate statistics like principal component analysis (PCA). Here, orthogonal means perpendicular. We determine orthogonality by taking the inner product of two vectors.\nTake two vectors \\(a\\) and \\(b\\), they are orthogonal if and only if\n\\(a \\perp b \\iff a \\cdot b = 0\\)\n\na <- c(3,-3)\nb <- c(3, 3)\n\nsum(a*b) # yep, a and b are orthogonal!\n\n[1] 0"
  },
  {
    "objectID": "lab01-lin-alg.html#resources",
    "href": "lab01-lin-alg.html#resources",
    "title": "Lab 01 Linear alg.",
    "section": "Resources",
    "text": "Resources\nHere are a few web resources for self learning if you wish to go farther\n3Blue1Brown Essence of linear algebra\nKhan Academy linear algebra series\nCoursera Mathematics for machine learning linear algebra course (Imperial College London - free to audit)\nMIT x18.06 Gilbert Strang’s famous linear algebra course"
  },
  {
    "objectID": "lab01-lin-alg.html#harper-adams-data-science",
    "href": "lab01-lin-alg.html#harper-adams-data-science",
    "title": "Lab 01 Linear alg.",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01-lin-alg.html#matrices",
    "href": "lab01-lin-alg.html#matrices",
    "title": "Lab 01 Linear alg.",
    "section": "3 Matrices",
    "text": "3 Matrices\n\n3.1 Description\nMatrices are described by the number of rows and columns they have. We may say a matrix \\(A\\) to have dimensions \\(r \\times c\\), (rows \\(\\times\\) columns).\n$ A =\n\\[\\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1c} \\\\ a_{21} & a_{22} & \\dots & a_{2c} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{r1} & a_{r2} & \\dots & a_{rc} \\end{bmatrix}\\]\n$\n\n# make matrix, vector assembled \"by column\"\nA <- matrix(c(4,3,6,2,7,4,4,5,4), ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    4    2    4\n[2,]    3    7    5\n[3,]    6    4    4\n\n\nA matrix can be constructed “by row” as well in R, with very different consequences.\n\n# make matrix, vector assembled \"by row\"\nB <- matrix(c(4,3,6,2,7,4,4,5,4), ncol = 3, byrow=T)\nB\n\n     [,1] [,2] [,3]\n[1,]    4    3    6\n[2,]    2    7    4\n[3,]    4    5    4\n\n\n\n\n3.2 Multiplying matrices\nFor a number \\(\\alpha\\) and a matrix \\(A\\), the product of \\(\\alpha A\\) is the matrix obtained by multiplying each element of \\(A\\) to \\(\\alpha\\).\n\\(\\alpha = 3\\)\n$A =\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\\\ 1 & 1 \\end{bmatrix}\\]\n$\n$3\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\\\ 1 & 1 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} 3 & 9 \\\\  6 & 12 \\\\ 3 & 3 \\end{bmatrix}\\]\n$\n\n# matrix multiplication in R\nalpha <- 3\nA <- matrix(c(1,3,2,4,1,1), byrow=T, ncol=2)\nalpha*A\n\n     [,1] [,2]\n[1,]    3    9\n[2,]    6   12\n[3,]    3    3\n\n\n\n\n3.3 Transpose for matrices\nMatrix trasposition works similarly to vector transpostiion and is also denoted by \\(\\intercal\\)\n$\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\\\ 1 & 1 \\end{bmatrix}\\]\n^ =\n\\[\\begin{bmatrix} 1 & 2 & 1 \\\\  3 & 4 & 1 \\end{bmatrix}\\]\n$\n\n# Matrix transpose in R\nA <- matrix(c(1,3,2,4,1,1), byrow=T, ncol=2)\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    3    4    1\n\n\n\n\n3.4 Sum of matrices\nLet \\(A\\) and \\(B\\) be matrices of dimensions \\(r \\times c\\). We sum the matrices together element-wise. The matrices must be of exactly the same dimensions.\n$\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\\\ 1 & 1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix} 7 & 1 \\\\  1 & 1 \\\\ 4 & 4 \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} 8 & 4 \\\\  3 & 5 \\\\ 5 & 5 \\end{bmatrix}\\]\n$\n\n\n# Adding matrices in R\nA <- matrix(c(1,3,2,4,1,1), byrow=3, ncol=2)\nB <- matrix(c(7,1,1,1,4,4), byrow=3, ncol=2)\n\nA +B\n\n     [,1] [,2]\n[1,]    8    4\n[2,]    3    5\n[3,]    5    5\n\n\n\n\n3.5 Multiplying matrix x vector\nLet \\(A\\) be an \\(r \\times c\\) matrix and ;et \\(B\\) be a column vector with \\(c\\) dimensions Note the number of elements in one dimension (here \\(c\\)) bust be the same.\n$\n\\[\\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1c} \\\\ a_{21} & a_{22} & \\dots & a_{2c} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{r1} & a_{r2} & \\dots & a_{rc} \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix} b_{1} \\\\  b_{2} \\\\  \\vdots \\\\ b_{c} \\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix} a_{11} b_{1} + a_{12} b_{2} +  \\dots + a_{1c}b_{c} \\\\ a_{21} b_{1} + a_{22} b_{2} + \\dots + a_{2c} b_{c} \\\\ \\vdots  \\\\ a_{r1} b_{1} + a_{r2} b_{2} +  \\dots   + a_{rc} b_{c} \\end{bmatrix}\\]\n$\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 7 + 3 \\cdot 1 \\\\ 2 \\cdot 7 + 4 \\cdot 1 \\\\ 1 \\cdot 7 + 1 \\cdot 1 \\end{bmatrix}\\begin{bmatrix} 10 \\\\ 18 \\\\ 8 \\end{bmatrix}\\)\n\n# Matrix x vector multiplication in R\nA <- matrix(c(1,3,2,4,1,1), byrow=3, ncol=2)\nC <- c(7,1)\n\n# NB the %*% syntax, correct for matrix x vector\nA %*% C\n\n     [,1]\n[1,]   10\n[2,]   18\n[3,]    8\n\n# NB this will also evaluate, but has a different result...\n# Can you tell what is going on here?\nA * C\n\n     [,1] [,2]\n[1,]    7    3\n[2,]    2   28\n[3,]    7    1\n\n\n\n\n3.6 Multiplying matrix x matrix\n(Here it gets tricky)\nLet \\(A\\) be an \\(r \\times c\\) matrix and \\(B\\) be a \\(c \\times t\\) matrix, where the number of columns in \\(A\\) is equal to the number of rows in \\(B\\).\n\n# Matrix x matrix multiplication in R\nA <- matrix(c(1, 3, 2, 2, 8, 9), ncol = 2)\nB <- matrix(c(5, 8, 4, 2), ncol = 2)\n\n# NB the %*% syntax\nA %*% B\n\n     [,1] [,2]\n[1,]   21    8\n[2,]   79   28\n[3,]   82   26\n\n\n\n\n3.7 Vectors as matrics\nVectors can be treated as matrices and in R can be coerced to matrix objects, where a column vector of length \\(r\\) becomes an \\(r \\times 1\\) matrix or a row vector of length \\(c\\) becomes a \\(1 \\times c\\) matrix\n\n# Vectors as matrices in R\n\n# Vanilla numeric vector\n(A <- c(4,5,8)); class(A)\n\n[1] 4 5 8\n\n\n[1] \"numeric\"\n\n# Column matrix\n(A <- matrix(c(4,5,8), nrow=3)); class(A)\n\n     [,1]\n[1,]    4\n[2,]    5\n[3,]    8\n\n\n[1] \"matrix\" \"array\" \n\n# Row matrix\n(A <- matrix(c(4,5,8), ncol=3)); class(A)\n\n     [,1] [,2] [,3]\n[1,]    4    5    8\n\n\n[1] \"matrix\" \"array\" \n\n\n\n\n3.8 Special matrics\n– Square matrix An n × n matrix\n– Symmetric matrix A is if \\(A = A^\\intercal\\).\n– 0-matrix A matrix with 0 on all entries, often written simply as 0.\n– 1-matrix A matrix with 1 on all entries, often written simply as J.\n– Diagonal matrix A square matrix with 0 on all off–diagonal entries and elements d1, d2, … , dn on the diagonal, often written diag{d1, d2, … , dn}\n– Identity matrix is one with with all 1s on the diagonal, denoted I and satisfies that IA = AI = A.\n\n# 0-matrix\nmatrix(0, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n# 1-matrix\nmatrix(1, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n\n# Diagonal matrix\ndiag(c(1, 2, 3))\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# Identity matrix\ndiag(1, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n# Note what happens when diag() is applied to a matrix\n(D <- diag(c(4,3,5)))\n\n     [,1] [,2] [,3]\n[1,]    4    0    0\n[2,]    0    3    0\n[3,]    0    0    5\n\ndiag(D)\n\n[1] 4 3 5\n\n(A <- matrix(c(1,3,2,2,6,8,9,3,4), ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    1    2    9\n[2,]    3    6    3\n[3,]    2    8    4\n\ndiag(A)\n\n[1] 1 6 4\n\n\n\n\n3.9 Inverse of a matrix\nThe inverse of an \\(n × n\\) matrix \\(A\\) is the \\(n × n\\) matrix \\(B\\) (which is which when multiplied with A gives the identity matrix I. That is, \\(AB = BA = I\\).\nThus\n\\(B\\) is the inverse of \\(A\\), written as \\(B = A^{−1}\\) and\n\\(A\\) is the inverse of \\(B\\), written as \\(A = B^{−1}\\)\nNumeric example\n$ A =\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\end{bmatrix}\\]\nB =\n\\[\\begin{bmatrix} -2 & 1.5 \\\\  1 & -0.5 \\end{bmatrix}\\]\n$\nWe can show \\(AB = BA = I\\), thus \\(B=A^{-1}\\)\n\n# Inverse of matrices\n\n(A <- matrix(c(1,3,2,4), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n(B <- matrix(c(-2,1.5,1,-0.5), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\nA%*%B\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nB%*%A == diag(1,2)\n\n     [,1] [,2]\n[1,] TRUE TRUE\n[2,] TRUE TRUE\n\n\n– Only square matrices can have an inverse, but not all square matrices have an inverse. – When the inverse exists, it is unique. – Finding the inverse of a large matrix A is numerically complicated (but computers do it for us).\n\n# Solving the inverse of a matrix in R using solve()\n\n(A <- matrix(c(1,3,2,4), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n(B <- solve(A))\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n# Prove the rule\nA %*% B\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "lab01-lin-alg.html#special-topics",
    "href": "lab01-lin-alg.html#special-topics",
    "title": "Lab 01 Linear alg.",
    "section": "4 Special topics",
    "text": "4 Special topics\n\n4.1 Solving linear equations\nMatrix math is related to math that can be used to solve linear equation systems. This is a very large topic and we will only briefly touch upon it, but it is core in statistics and in machine learning. We can sometimes ignore the details, but awareness of this area of math will likely be beneficial.\nConsider these two linear equations\n\\(x_1 + 3x_2 = 7\\)\n\\(2x_1 + 4x_2 = 10\\)\nWe can write this “system” of equations in matrix form, from which is derived the notation for statistical linear models. Let’s define the matrices \\(A\\), \\(x\\) and \\(b\\) as:\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 10 \\end{bmatrix}\\), i.e. \\(Ax = b\\)\nBecause \\(A^{-1}A = I\\) and \\(Ix=x\\):\n\\(x = A^{-1}b = \\begin{bmatrix} -2 & 1.5 \\\\ 1 & -0.5 \\end{bmatrix} \\begin{bmatrix} 7 \\\\ 10 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\)\nThis way of thinking is the fountation of the linear model and we can exploit it to isolate and “solve” for the x values. E.g. we can isolate and solve for \\(x_2\\) as:\n\\(x_2 = \\frac{7}{3}-\\frac{1}{3}x_1\\), \\(x_2 = \\frac{10}{4}-\\frac{2}{4}x_1\\)\nNow we can graphically represent these equations, which are two lines and which demonstrate the solutions\n\n\n\n\n\nThe lines represent the solved equations above, and it can be seen that they cross at a single point, the solutions for \\(x_1\\) and \\(x_2\\), \\(x_1 = 1\\) and \\(x_2=2\\), respectively.\n\nA <- matrix(c(1, 2, 3, 4), ncol = 2)\nb <- c(7, 10)\n(x <- solve(A) %*% b)\n\n     [,1]\n[1,]    1\n[2,]    2\n\n\nWhile in this example we see exactly 1 solution, there are several possibilities in general:\n\nExactly one solution – when the lines intersect in one point\nNo solutions – when the lines are parallel but not identical\nInfinitely many solutions – when the lines coincide.\n\n\n\n4.2 Matrix equalities\nHere are a few additional properties of matrices\n\\((A + B)^\\intercal = A^\\intercal + B^\\intercal\\)\n\\((AB)^\\intercal = B^\\intercal A^\\intercal\\)\n\\(A(B + C) = AB + AC\\)\n\\(AB = AC 6) B = C\\)\nIn generel \\(AB \\neq BA\\)\n\\(AI = IA = A\\)\nIf \\(\\alpha\\) is a number then \\(\\alpha AB = A(\\alpha B)\\)\n\n\n4.3 Least squares\nConsider the following paired values\n\ndf <- data.frame(x=c(1,2,3,4,5),\n                 y=c(3.7, 4.2,4.9,5.7,6.0))\ndf\n\n  x   y\n1 1 3.7\n2 2 4.2\n3 3 4.9\n4 4 5.7\n5 5 6.0\n\n\nWe can plot these values and note that the relationship is approvimately linear (i.e., a straight line can be drawn that comes close to all the values)\n\n\n\n\n\nWe can notate the linear relationship between \\(x\\) and \\(y\\) with\n\\(y_i = \\beta_0 +\\beta_1x_i\\) for \\(i = 1, \\dots, 5\\)\nThis can be written in matrix form\n\\(y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_5 \\end{bmatrix} \\approx \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\1 & x_5 \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1\\end{bmatrix} = \\mathbf{X \\beta}\\)\nGiven that we have vectors \\(y\\) and \\(x\\), how can we find the vector \\(\\mathbf{\\beta}\\) to satisfy this equality such that \\(y=X \\beta\\)?\nActually, the answer is no because this would require the points to lie on an exactly straight line! However, it may be useful to find an estimate of \\(\\beta\\) such that \\(\\mathbf{X \\beta}\\) is “as close to \\(y\\) as possible”. Let’s call this estimate \\(\\hat{\\beta}\\)\nTo do this we would like to find \\(\\hat{\\beta}\\) that minimizes the equation\n\\(e=y- \\mathbf{X \\beta}\\)\nThe solution to this problem is the basis of linear regression and other linear statistical models:\n\\(\\hat{\\beta} = (X^\\intercal X)^{-1} X^\\intercal y\\)\n\ny <- c(3.7, 4.2,4.9,5.7,6.0)\nx <- c(1,2,3,4,5)\nX <- matrix(c(rep(1,5), x), ncol=2, byrow=F)\ncolnames(X) <- c('','x')\n\ny\n\n[1] 3.7 4.2 4.9 5.7 6.0\n\nX\n\n       x\n[1,] 1 1\n[2,] 1 2\n[3,] 1 3\n[4,] 1 4\n[5,] 1 5\n\n# this solves \"Beta hat\"\n# NB, the intercept and slope coeeficients for linear regression!\n(beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y)\n\n  [,1]\n  3.07\nx 0.61"
  },
  {
    "objectID": "lab01-lin-alg.html#exercises",
    "href": "lab01-lin-alg.html#exercises",
    "title": "Lab 01 Linear alg.",
    "section": "5 Exercises",
    "text": "5 Exercises\n\n5.1 The bird table\nOn a sunny day, two tables are standing in an English country garden. On each table birds of unknown species are sitting having the time of their lives. A bird from the first table says to those on the second table: “Hi – if one of you come to our table then there will be the same number of us on each table”. “Yeah, right”, says a bird from the second table, “but if one of you comes to our table, then we will be twice as many on our table as on yours”.\nQuestion: How many birds are on each table?\n\nWrite up two equations with two unknowns\nSolve these equations using the methods you have learned from linear algebra\nSimply finding the solution by trial–and–error is invalid"
  },
  {
    "objectID": "lec00-module-intro.html#c7081-statistical-analysis-for-data-science",
    "href": "lec00-module-intro.html#c7081-statistical-analysis-for-data-science",
    "title": "c7081 Lec00 Module Introduction",
    "section": "C7081 Statistical Analysis for Data Science",
    "text": "C7081 Statistical Analysis for Data Science\n\nEd Harris"
  },
  {
    "objectID": "lec00-module-intro.html#section",
    "href": "lec00-module-intro.html#section",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "",
    "text": "C7081 Statistical Analysis for Data Science\nEd Harris"
  },
  {
    "objectID": "lec00-module-intro.html#module-overview",
    "href": "lec00-module-intro.html#module-overview",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "Module overview",
    "text": "Module overview\n\n\n\n\n“I am always ready to learn although I do not always like being taught.”\n\n-Winston Churchill"
  },
  {
    "objectID": "lec00-module-intro.html#module-overview-1",
    "href": "lec00-module-intro.html#module-overview-1",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "Module overview",
    "text": "Module overview\n\n\nOne week intensive format\nDaily catch-up meetings\nRecorded lectures, readings\nLabs are critical"
  }
]