[
  {
    "objectID": "lab01-lin-alg.html",
    "href": "lab01-lin-alg.html",
    "title": "Lab 01 Linear alg.",
    "section": "",
    "text": "Linear algebra is the (math) foundation of statistics and data science. While it is possible to practice data analysis without a robust knowledge of algebra, a little bit helps. The purpose here is to highlight and review the key linear algebra concepts and to demonstrate a few examples. By the end of this lab you should be able to:\n\nDescribe the structure of vectors and matrices\nPerform math functions with linear algebra structures\nDemonstrate awareness of linear algebra utility"
  },
  {
    "objectID": "lab01-lin-alg.html#vectors",
    "href": "lab01-lin-alg.html#vectors",
    "title": "Lab 01 Linear alg.",
    "section": "2 Vectors",
    "text": "2 Vectors\n\n2.1 The basic vector concept\nVectors can be conceptualized as a list of numerical values (elements) that may be arranged in columns or rows. A formal difference between column and row vectors is the notation for their arrangement, where a vector has n elements, a row vector is a matrix with \\([1 \\times n]\\) elements; a column vector has \\([n \\times 1]\\) elements.\nColumn vector:\n\\(a=\\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\)\nRow vector:\n\\(b=\\begin{bmatrix} 2, 1, 3 \\end{bmatrix}\\)\nVectors have a specific order such that:\n\\((2,1,3) \\neq (1,2,3)\\)\nWe can generalize the notation for a vector containing n elements as an n-vector such that:\n\\(a=\\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix}\\)\nwhere each element \\(a_i\\) is a numerical value and the vector can be written as \\(a=(a_1,\\dots,a_n)\\).\nWe can represent vectors graphically, e.g. here is an example in R graphing 2 2-vectors.There are some conventions in geometry and math notation here, that are not necessarily the same as the way we store data structures in a programming language…\n\n# imagine two vectors that each contain the x,y coordinates of a point\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\n\n\n\n\n\n\nVectors in R are always printed in the ‘row format’, regardless of math notation.\n\na <- c(4,2,3)\na\n\n[1] 4 2 3\n\n\n\n\n2.2 Transposing\nTansposing a vector is when a column or row vector is turned into the opposite orientation. The transpose is notated with the symbol \\(\\intercal\\)\nColumn to row format\n\\(\\begin{bmatrix} 4 \\\\ 8 \\\\ 5 \\end{bmatrix} ^ \\intercal = [4, 8, 5]\\)\nRow to column format\n\\([4, 8, 5] ^ \\intercal = \\begin{bmatrix} 4 \\\\ 8 \\\\ 5 \\end{bmatrix}\\)\n\n# transpose in R\na <- c(5,7,6)\n\n# the t() function forces the object as a matrix\nt(a)\n\n     [,1] [,2] [,3]\n[1,]    5    7    6\n\n# multiple transpose, just to demonstrate\nt(t(a))\n\n     [,1]\n[1,]    5\n[2,]    7\n[3,]    6\n\n\n\n\n2.3 Multiplication\nA number \\(b\\) and a vector \\(a\\) can be multiplied together\n\\(b \\times a =\\begin{bmatrix} b \\times a_1 \\\\ b \\times a_2 \\\\ \\vdots \\\\ b \\times a_n \\end{bmatrix}\\)\nThus\n\\(5 \\times \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}\\)\n\n# vector multiplication in R\na <- c(2,4,5)\nb <- 3\n\na*b\n\n[1]  6 12 15\n\n\nGraphing vector multiplication\n\n# our 2 vectors from before\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\n\n\n\n\n\n\n\n\n2.4 Addition\nLet \\(a\\) and \\(b\\) be n-vectors, where \\(a\\) and \\(b\\) are of the same dimensions.\n\\(a + b = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} = \\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix} = b+a\\)\nWith numbers\n\\(a + b = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 2 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 5 \\\\ 13 \\end{bmatrix}\\)\n\n# vector addition in R\na <- c(3, 5, 1)\nb <- c(14, 3, 5)\n\na + b\n\n[1] 17  8  6\n\n\n\n# our 2 vectors from before\nvec1 <- c(2,2)\nvec2 <- c(1,-0.5)\nvec3 <- vec1 + vec2\n\n\n\n\n\n\n\n\n2.5 Vector inner product\nThe inner product of a vector is obtained by multiplying two vectors and summing the result (NB this is sometimes called the dot product).\n\\(\\sum a*b = a \\cdot b = a_1b_1 + \\dots + a_nb_n\\)\nwith numbers\n\\(\\sum (\\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\end{bmatrix} \\times \\begin{bmatrix} 4 \\\\ 2 \\\\ 8 \\end{bmatrix}) = \\sum \\begin{bmatrix} 4 \\\\ 6 \\\\ 40 \\end{bmatrix} == 50\\)\n\n# dot product in R\na <- c(1,3,5)\nb <- c(4,2,8)\n\nsum(a * b)\n\n[1] 50\n\n\n\n# alternative syntax for the dot product\na %*% b\n\n     [,1]\n[1,]   50\n\n\n\n\n2.6 Magnitude (aka the “norm”) of a vector\nThere are several ways to measure the “bigness” of a vector, sometimes called the norms. Although we will not go into detail here, there are two types of norm to be aware of. These may seem a little esoteric for our purposes here, but they are used “under the bonnet” for many statistical and machine learning calculations (thus, you may encounter them and should probably be aware of them).\nL1 norm (aka the outer norm) - this is the overall absolute magnitude of vector values\nL2 norm (aka the inner norm) this is the linear (“Euclidean”) distance of the vector from the origin (the zero value in n-dimensional space).\n\nL1 norm\nThe L1 norm is calculated by summing the absolute value of all vector elements.\nTake a vector \\(a = (2, -4, 5)\\)\n\\(||a||_1 = \\sum(|a_1|+ \\dots + |a_n|)\\)\n\\(||a||_1 = (2 + 4 + 5) = 11\\)\n\n\nL2 norm\nThe L2 norm is calculated by taking the square root of the summed values of the squared values of each element of a vector.\nTake a vector \\(b = (-1, 0, 3)\\)\n\\(||b||_2 = \\sqrt(b_1^2+ \\dots + b_n^2)\\)\n\\(||b||_2 = \\sqrt(1 + 0 + 9) = 3.16\\)\n\n# norms in R\na <- c(2, -4, 5)\nb <- c(-1, 0, 3)\n\nsum(abs(a)) # L1\n\n[1] 11\n\nsqrt(sum(b^2)) # L2\n\n[1] 3.162278\n\n# alternative calculation using the norm() function\n# ?norm\n\nnorm(matrix(a), type = \"O\") # L1\n\n[1] 11\n\nnorm(matrix(b), type = \"2\") # L2\n\n[1] 3.162278\n\n\n\n\n\n2.7 Special vectors\nThere are a few special cases of vectors we may encounter (but which are certainly there “doing work” for us), like the 0-vector and the 1-vector. These are simply vectors where all values assume either zero or one, respectively. These are often used in linear models to encode data for matrix calculations (but we will leave it at that for now).\n\n# 0-matrix, n=10\nrep(0, 10)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n# 1-matrix, n=8\nrep(1, 8)\n\n[1] 1 1 1 1 1 1 1 1\n\n\n\n\n2.8 Orthogonal vectors\nOrthogonal vectors are used in a number of statistical methods, e.g. multivariate statistics like principal component analysis (PCA). Here, orthogonal means perpendicular. We determine orthogonality by taking the inner product of two vectors.\nTake two vectors \\(a\\) and \\(b\\), they are orthogonal if and only if\n\\(a \\perp b \\iff a \\cdot b = 0\\)\n\na <- c(3,-3)\nb <- c(3, 3)\n\nsum(a*b) # yep, a and b are orthogonal!\n\n[1] 0"
  },
  {
    "objectID": "lab01-lin-alg.html#matrices",
    "href": "lab01-lin-alg.html#matrices",
    "title": "Lab 01 Linear alg.",
    "section": "3 Matrices",
    "text": "3 Matrices\n\n3.1 Description\nMatrices are described by the number of rows and columns they have. We may say a matrix \\(A\\) to have dimensions \\(r \\times c\\), (rows \\(\\times\\) columns).\n\\(A = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1c} \\\\ a_{21} & a_{22} & \\dots & a_{2c} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{r1} & a_{r2} & \\dots & a_{rc} \\end{bmatrix}\\)\n\n# make matrix, vector assembled \"by column\"\nA <- matrix(c(4,3,6,2,7,4,4,5,4), ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    4    2    4\n[2,]    3    7    5\n[3,]    6    4    4\n\n\nA matrix can be constructed “by row” as well in R, with very different consequences.\n\n# make matrix, vector assembled \"by row\"\nB <- matrix(c(4,3,6,2,7,4,4,5,4), ncol = 3, byrow=T)\nB\n\n     [,1] [,2] [,3]\n[1,]    4    3    6\n[2,]    2    7    4\n[3,]    4    5    4\n\n\n\n\n3.2 Multiplying matrices\nFor a number \\(\\alpha\\) and a matrix \\(A\\), the product of \\(\\alpha A\\) is the matrix obtained by multiplying each element of \\(A\\) to \\(\\alpha\\).\n\\(\\alpha = 3\\)\n\\(A = \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix}\\)\n\\(3 \\times\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 9 \\\\ 6 & 12 \\\\ 3 & 3 \\end{bmatrix}\\)\n\n# matrix multiplication in R\nalpha <- 3\nA <- matrix(c(1,3,2,4,1,1), byrow=T, ncol=2)\nalpha*A\n\n     [,1] [,2]\n[1,]    3    9\n[2,]    6   12\n[3,]    3    3\n\n\n\n\n3.3 Transpose for matrices\nMatrix trasposition works similarly to vector transpostiion and is also denoted by \\(\\intercal\\)\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix}^ \\intercal = \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 4 & 1 \\end{bmatrix}\\)\n\n# Matrix transpose in R\nA <- matrix(c(1,3,2,4,1,1), byrow=T, ncol=2)\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    2    1\n[2,]    3    4    1\n\n\n\n\n3.4 Sum of matrices\nLet \\(A\\) and \\(B\\) be matrices of dimensions \\(r \\times c\\). We sum the matrices together element-wise. The matrices must be of exactly the same dimensions.\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix} - \\begin{bmatrix} 7 & 1 \\\\ 1 & 1 \\\\ 4 & 4 \\end{bmatrix} = \\begin{bmatrix} 8 & 4 \\\\ 3 & 5 \\\\ 5 & 5 \\end{bmatrix}\\)\n\n# Adding matrices in R\nA <- matrix(c(1,3,2,4,1,1), byrow=3, ncol=2)\nB <- matrix(c(7,1,1,1,4,4), byrow=3, ncol=2)\n\nA +B\n\n     [,1] [,2]\n[1,]    8    4\n[2,]    3    5\n[3,]    5    5\n\n\n\n\n3.5 Multiplying matrix x vector\nLet \\(A\\) be an \\(r \\times c\\) matrix and let \\(B\\) be a column vector with \\(c\\) dimensions Note the number of elements in one dimension (here \\(c\\)) must be the same.\n\\(\\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1c} \\\\ a_{21} & a_{22} & \\dots & a_{2c} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{r1} & a_{r2} & \\dots & a_{rc} \\end{bmatrix} \\times \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{c} \\end{bmatrix} = \\begin{bmatrix} a_{11} b_{1} + a_{12} b_{2} + \\dots + a_{1c}b_{c} \\\\ a_{21} b_{1} + a_{22} b_{2} + \\dots + a_{2c} b_{c} \\\\ \\vdots \\\\ a_{r1} b_{1} + a_{r2} b_{2} + \\dots + a_{rc} b_{c} \\end{bmatrix}\\)\nFor example:\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\\\ 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 7 + 3 \\cdot 1 \\\\ 2 \\cdot 7 + 4 \\cdot 1 \\\\ 1 \\cdot 7 + 1 \\cdot 1 \\end{bmatrix}\\begin{bmatrix} 10 \\\\ 18 \\\\ 8 \\end{bmatrix}\\)\n\n# Matrix x vector multiplication in R\nA <- matrix(c(1,3,2,4,1,1), byrow=3, ncol=2)\nC <- c(7,1)\n\n# NB the %*% syntax, correct for matrix x vector\nA %*% C\n\n     [,1]\n[1,]   10\n[2,]   18\n[3,]    8\n\n# NB this will also evaluate, but has a different result...\n# Can you tell what is going on here?\nA * C\n\n     [,1] [,2]\n[1,]    7    3\n[2,]    2   28\n[3,]    7    1\n\n\n\n\n3.6 Multiplying matrix x matrix\n(Here it gets tricky)\nLet \\(A\\) be an \\(r \\times c\\) matrix and \\(B\\) be a \\(c \\times t\\) matrix, where the number of columns in \\(A\\) is equal to the number of rows in \\(B\\).\n\n# Matrix x matrix multiplication in R\nA <- matrix(c(1, 3, 2, 2, 8, 9), ncol = 2)\nB <- matrix(c(5, 8, 4, 2), ncol = 2)\n\n# NB the %*% syntax\nA %*% B\n\n     [,1] [,2]\n[1,]   21    8\n[2,]   79   28\n[3,]   82   26\n\n\n\n\n3.7 Vectors as matrics\nVectors can be treated as matrices and in R can be coerced to matrix objects, where a column vector of length \\(r\\) becomes an \\(r \\times 1\\) matrix or a row vector of length \\(c\\) becomes a \\(1 \\times c\\) matrix\n\n# Vectors as matrices in R\n\n# Vanilla numeric vector\n(A <- c(4,5,8)); class(A)\n\n[1] 4 5 8\n\n\n[1] \"numeric\"\n\n# Column matrix\n(A <- matrix(c(4,5,8), nrow=3)); class(A)\n\n     [,1]\n[1,]    4\n[2,]    5\n[3,]    8\n\n\n[1] \"matrix\" \"array\" \n\n# Row matrix\n(A <- matrix(c(4,5,8), ncol=3)); class(A)\n\n     [,1] [,2] [,3]\n[1,]    4    5    8\n\n\n[1] \"matrix\" \"array\" \n\n\n\n\n3.8 Special matrics\n– Square matrix An n × n matrix\n– Symmetric matrix A is if \\(A = A^\\intercal\\).\n– 0-matrix A matrix with 0 on all entries, often written simply as 0.\n– 1-matrix A matrix with 1 on all entries, often written simply as J.\n– Diagonal matrix A square matrix with 0 on all off–diagonal entries and elements d1, d2, … , dn on the diagonal, often written diag{d1, d2, … , dn}\n– Identity matrix is one with with all 1s on the diagonal, denoted I and satisfies that IA = AI = A.\n\n# 0-matrix\nmatrix(0, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n# 1-matrix\nmatrix(1, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n\n# Diagonal matrix\ndiag(c(1, 2, 3))\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    2    0\n[3,]    0    0    3\n\n# Identity matrix\ndiag(1, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n# Note what happens when diag() is applied to a matrix\n(D <- diag(c(4,3,5)))\n\n     [,1] [,2] [,3]\n[1,]    4    0    0\n[2,]    0    3    0\n[3,]    0    0    5\n\ndiag(D)\n\n[1] 4 3 5\n\n(A <- matrix(c(1,3,2,2,6,8,9,3,4), ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    1    2    9\n[2,]    3    6    3\n[3,]    2    8    4\n\ndiag(A)\n\n[1] 1 6 4\n\n\n\n\n3.9 Inverse of a matrix\nThe inverse of an \\(n × n\\) matrix \\(A\\) is the \\(n × n\\) matrix \\(B\\) (which is which when multiplied with A gives the identity matrix I. That is, \\(AB = BA = I\\).\nThus\n\\(B\\) is the inverse of \\(A\\), written as \\(B = A^{−1}\\) and\n\\(A\\) is the inverse of \\(B\\), written as \\(A = B^{−1}\\)\nNumeric example\n$ A =\n\\[\\begin{bmatrix} 1 & 3 \\\\  2 & 4 \\end{bmatrix}\\]\n$\n$ B =\n\\[\\begin{bmatrix} -2 & 1.5 \\\\  1 & -0.5 \\end{bmatrix}\\]\n$\nWe can show \\(AB = BA = I\\), thus \\(B=A^{-1}\\)\n\n# Inverse of matrices\n\n(A <- matrix(c(1,3,2,4), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n(B <- matrix(c(-2,1.5,1,-0.5), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\nA%*%B\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nB%*%A == diag(1,2)\n\n     [,1] [,2]\n[1,] TRUE TRUE\n[2,] TRUE TRUE\n\n\n– Only square matrices can have an inverse, but not all square matrices have an inverse. – When the inverse exists, it is unique. – Finding the inverse of a large matrix A is numerically complicated (but computers do it for us).\n\n# Solving the inverse of a matrix in R using solve()\n\n(A <- matrix(c(1,3,2,4), ncol=2, byrow=T))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n(B <- solve(A))\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\n# Prove the rule\nA %*% B\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "lab01-lin-alg.html#special-topics",
    "href": "lab01-lin-alg.html#special-topics",
    "title": "Lab 01 Linear alg.",
    "section": "4 Special topics",
    "text": "4 Special topics\n\n4.1 Solving linear equations\nMatrix math is related to math that can be used to solve linear equation systems. This is a very large topic and we will only briefly touch upon it, but it is core in statistics and in machine learning. We can sometimes ignore the details, but awareness of this area of math will likely be beneficial.\nConsider these two linear equations\nEq 1: \\(x_1 + 3x_2 = 7\\)\nEq 2: \\(2x_1 + 4x_2 = 10\\)\nWe can write this “system” of equations in matrix form, from which is derived the notation for statistical linear models. Let’s define the matrices \\(A\\), \\(x\\) and \\(b\\) as:\n\\(\\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 10 \\end{bmatrix}\\), i.e. \\(Ax = b\\)\nBecause \\(A^{-1}A = I\\) and \\(Ix=x\\):\n\\(x = A^{-1}b = \\begin{bmatrix} -2 & 1.5 \\\\ 1 & -0.5 \\end{bmatrix} \\begin{bmatrix} 7 \\\\ 10 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\)\nThis way of thinking is the foundation of the linear model and we can exploit it to isolate and “solve” for the x values. E.g. we can isolate and solve for \\(x_2\\) as:\nEq 1 (rerarranged): \\(x_2 = \\frac{7}{3}-\\frac{1}{3}x_1\\)\nEq 2 (rerarranged): \\(x_2 = \\frac{10}{4}-\\frac{2}{4}x_1\\)\n\\(x_2 = \\frac{7}{3}-\\frac{1}{3}x_1\\)\n\\(x_2 = \\frac{10}{4}-\\frac{2}{4}x_1\\)\nNow we can graphically represent these equations, which are two lines and which demonstrate the solutions\n\nplot(x=NULL, y=NULL,\n     xlim = c(-1,3), ylim = c(-1,3),\n     pch = 16, \n     xlab=expression('x'[1]), \n     ylab=expression('x'[2]))\nabline(h=c(-1:3),v=c(-1:3),\n       lty=2, col='green3')\nabline(h=0, v=0, lwd=2)\n\nabline(a=7/3, b=-1/3, col=\"red\", lwd=2)\nabline(a=10/4, b=-2/4, col=\"red\", lwd=2)\n\n\n\n\nThe lines represent the solved equations above, and it can be seen that they cross at a single point, the solutions for \\(x_1\\) and \\(x_2\\), \\(x_1 = 1\\) and \\(x_2=2\\), respectively.\n\nA <- matrix(c(1, 2, 3, 4), ncol = 2)\nb <- c(7, 10)\n(x <- solve(A) %*% b)\n\n     [,1]\n[1,]    1\n[2,]    2\n\n\nWhile in this example we see exactly 1 solution, there are several possibilities in general:\n\nExactly one solution – when the lines intersect in one point\nNo solutions – when the lines are parallel but not identical\nInfinitely many solutions – when the lines coincide.\n\n\n\n4.2 Matrix equalities\nHere are a few additional properties of matrices\n\\((A + B)^\\intercal = A^\\intercal + B^\\intercal\\)\n\\((AB)^\\intercal = B^\\intercal A^\\intercal\\)\n\\(A(B + C) = AB + AC\\)\n\\(AB = AC \\not\\Rightarrow B = C\\)\n\\(AB \\neq BA\\) (in general)\n\\(AI = IA = A\\)\nIf \\(\\alpha\\) is a number then \\(\\alpha AB = A(\\alpha B)\\)\n\n\n4.3 Least squares\nConsider the following paired values\n\ndf <- data.frame(x=c(1,2,3,4,5),\n                 y=c(3.7, 4.2,4.9,5.7,6.0))\ndf\n\n  x   y\n1 1 3.7\n2 2 4.2\n3 3 4.9\n4 4 5.7\n5 5 6.0\n\n\nWe can plot these values and note that the relationship is approvimately linear (i.e., a straight line can be drawn that comes close to all the values)\n\n\n\n\n\nWe can notate the linear relationship between \\(x\\) and \\(y\\) with\n\\(y_i = \\beta_0 +\\beta_1x_i\\) for \\(i = 1, \\dots, 5\\)\nThis can be written in matrix form\n\\(y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_5 \\end{bmatrix} \\approx \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\1 & x_5 \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1\\end{bmatrix} = \\mathbf{X \\beta}\\)\nGiven that we have vectors \\(y\\) and \\(x\\), how can we find the vector \\(\\mathbf{\\beta}\\) to satisfy this equality such that \\(y=X \\beta\\)?\nActually, the answer is no because this would require the points to lie on an exactly straight line! However, it may be useful to find an estimate of \\(\\beta\\) such that \\(\\mathbf{X \\beta}\\) is “as close to \\(y\\) as possible”. Let’s call this estimate \\(\\hat{\\beta}\\)\nTo do this we would like to find \\(\\hat{\\beta}\\) that minimizes the equation\n\\(e=y- \\mathbf{X \\beta}\\)\nThe solution to this problem is the basis of linear regression and other linear statistical models:\n\\(\\hat{\\beta} = (X^\\intercal X)^{-1} X^\\intercal y\\)\n\ny <- c(3.7, 4.2,4.9,5.7,6.0)\nx <- c(1,2,3,4,5)\nX <- matrix(c(rep(1,5), x), ncol=2, byrow=F)\ncolnames(X) <- c('','x')\n\ny\n\n[1] 3.7 4.2 4.9 5.7 6.0\n\nX\n\n       x\n[1,] 1 1\n[2,] 1 2\n[3,] 1 3\n[4,] 1 4\n[5,] 1 5\n\n# this solves \"Beta hat\"\n# NB, the intercept and slope coeeficients for linear regression!\n(beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y)\n\n  [,1]\n  3.07\nx 0.61"
  },
  {
    "objectID": "lab01-lin-alg.html#exercises",
    "href": "lab01-lin-alg.html#exercises",
    "title": "Lab 01 Linear alg.",
    "section": "5 Exercises",
    "text": "5 Exercises\n\n5.1 The bird table\nOn a sunny day, two tables are standing in an English country garden. On each table are *birds of unknown species are sitting having the time of their lives. A bird from the first table says to those on the second table: “Hi – if one of you come to our table then there will be the same number of us on each table”. “Yeah, right”, says a bird from the second table, “but if one of you comes to our table, then we will be twice as many on our table as on yours”.\n*(because this is a maths question, here we have the ability to assume by birds we may mean something theoretical, and the count of which may assume any value from the infinite set of integers positive or negative…)\nQuestion: How many birds are on each table?\n\nWrite up two equations with two unknowns\nSolve these equations using the methods you have learned from linear algebra\nSimply finding the solution by trial–and–error is invalid (and will probably lead to frustration, especially if the question is taken literally - i.e., USE THE TOOLS we practiced)"
  },
  {
    "objectID": "lab01-lin-alg.html#resources",
    "href": "lab01-lin-alg.html#resources",
    "title": "Lab 01 Linear alg.",
    "section": "Resources",
    "text": "Resources\nHere are a few web resources for self learning if you wish to go farther\n3Blue1Brown Essence of linear algebra\nKhan Academy linear algebra series\nCoursera Mathematics for machine learning linear algebra course (Imperial College London - free to audit)\nMIT x18.06 Gilbert Strang’s famous linear algebra course"
  },
  {
    "objectID": "lab01-lin-alg.html#harper-adams-data-science",
    "href": "lab01-lin-alg.html#harper-adams-data-science",
    "title": "Lab 01 Linear alg.",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab09-svm.html",
    "href": "lab09-svm.html",
    "title": "Lab 09 SVM",
    "section": "",
    "text": "We use the {e1071} library in R to demonstrate the support vector classifier and the SVM. Another option is the {LiblineaR} library, which is useful for very large linear problems.\n\n\nThe {e1071} library contains implementations for a number of statistical learning methods. In particular, the svm() function can be used to fit a support vector classifier when the argument kernel = \"linear\" is used. A cost argument allows us to specify the cost of a violation to the margin. When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n\n\n\nWe now use the svm() function to fit the support vector classifier for a given value of the cost parameter. Here we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decision boundary. We begin by generating the observations, which belong to two classes, and checking whether the classes are linearly separable.\n\nset.seed(1)\nx <- matrix(rnorm(20 * 2), ncol = 2)\ny <- c( rep (-1, 10) , rep (1 , 10) )\nx[y == 1, ] <- x[ y == 1, ] + 1\nplot(x, col = (3 - y), pch=16)\n\n\n\n\nThey are not. Next, we fit the support vector classifier. Note that in order for the svm() function to perform classification (as opposed to SVM-based regression), we must encode the response as a factor variable. We now create a data frame with the response coded as a factor.\n\ndat <- data.frame(x = x, y = as.factor(y)) \nlibrary(e1071)\nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\", \n              cost = 10 , scale = FALSE )\n\nThe argument scale = FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one; depending on the application, one might prefer to use scale = TRUE.\nWe can now plot the support vector classifier obtained:\n\nplot(svmfit, dat) # first peek, not great\n\n\n\n\nNote that the two arguments to the SVM plot() function are the output of the call to svm(), as well as the data used in the call to svm(). The region of feature space that will be assigned to the 1 class is shown in light yellow, and the region that will be assigned to the +1 class is shown in red. The decision boundary between the two classes is linear (because we used the argument kernel = \"linear\"), though due to the way in which the plotting function is implemented in this library the decision boundary looks somewhat jagged in the plot. (Note that here the second feature is plotted on the x-axis and the first feature is plotted on the y-axis, in contrast to the behavior of the usual plot() function in R.) The support vectors are plotted as crosses and the remaining observations are plotted as circles; we see here that there are seven support vectors. We can determine their identities as follows:\n\nsvmfit$index\n\n[1]  1  2  5  7 14 16 17\n\n\nWe can obtain some basic information about the support vector classifier fit using the summary() command:\n\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  7\n\n ( 4 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\nThis tells us, for instance, that a linear kernel was used with cost = 10, and that there were seven support vectors, four in one class and three in the other.\nWhat if we instead used a smaller value of the cost parameter?\n\nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\",\n              cost = 0.1 , scale = FALSE )\nplot(svmfit, dat) \n\n\n\nsvmfit$index\n\n [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20\n\n\nNow that a smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider. Unfortunately, the svm() function does not explicitly output the coeﬃcients of the linear decision boundary obtained when the support vector classifier is fit, nor does it output the width of the margin\n\n\n\nThe {e1071} library includes a built-in function, tune(), to perform cross- validation. By default, tune() performs ten-fold cross-validation on a set of models of interest. In order to use this function, we pass in relevant information about the set of models that are under consideration. The following command indicates that we want to compare SVMs with a linear kernel, using a range of values of the cost parameter.\n\nset.seed(1)\ntune.out <- tune(svm, y ~ ., data = dat , kernel = \"linear\",\n                 ranges = list(cost = c(0.001, 0.01, 0.1,\n                                        1, 5, 10, 100)))\n\nWe can easily access the cross-validation errors for each of these models using the summary() command:\n\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  0.1\n\n- best performance: 0.05 \n\n- Detailed performance results:\n   cost error dispersion\n1 1e-03  0.55  0.4377975\n2 1e-02  0.55  0.4377975\n3 1e-01  0.05  0.1581139\n4 1e+00  0.15  0.2415229\n5 5e+00  0.15  0.2415229\n6 1e+01  0.15  0.2415229\n7 1e+02  0.15  0.2415229\n\n\n\n\n\nWe see that cost = 0.1 results in the lowest cross-validation error rate. The tune() function stores the best model obtained, which can be accessed as follows:\n\nbestmod <- tune.out$best.model\nsummary(bestmod) \n\n\nCall:\nbest.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n    0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.1 \n\nNumber of Support Vectors:  16\n\n ( 8 8 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\n<<<<<<< Updated upstream ### Testing with predict()` ======= ### Testing with \\predict()` >>>>>>> Stashed changes\nThe predict() function can be used to predict the class label on a set of test observations, at any given value of the cost parameter. We begin by generating a test data set.\n\nxtest <- matrix(rnorm(20 * 2), ncol = 2)\nytest <- sample(c(-1, 1), 20, rep = TRUE)\nxtest[ ytest == 1, ] <- xtest[ytest == 1, ] + 1\ntestdat <- data.frame(x = xtest, y = as.factor(ytest))\n\nNow we predict the class labels of these test observations. Here we use the best model obtained through cross-validation in order to make predictions.\n\nypred <- predict(bestmod, testdat)\n\n#our own confusion matrix\ntable(predict = ypred, truth = testdat$y)  #not bad!\n\n       truth\npredict -1 1\n     -1  9 1\n     1   2 8\n\n\nThus, with this value of cost, 17 of the test observations are correctly classified. What if we had instead used cost = 0.01?\n\nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\",\n              cost = .01, scale = FALSE)\nypred <- predict(svmfit, testdat)\n\ntable(predict = ypred, truth = testdat$y) # not as good\n\n       truth\npredict -1  1\n     -1 11  6\n     1   0  3\n\n\n<<<<<<< Updated upstream\nIn this case three additional observations are misclassified.\n\n\n\nNow consider a situation in which the two classes are linearly separable. Then we can find a separating hyperplane using the svm() function. We first further separate the two classes in our simulated data so that they are linearly separable:\n\nx[y == 1, ] <- x[y == 1, ] + 0.5\nplot (x, col = (y + 5) / 2, pch = 16)\n\n\n\n\nNow the observations are just barely linearly separable. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified.\n\ndat <- data.frame(x = x, y = as.factor(y)) \nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1e5 )\n\nIn this case three additional observations are misclassified.\n\n\n\nNow consider a situation in which the two classes are linearly separable. Then we can find a separating hyperplane using the svm() function. We first further separate the two classes in our simulated data so that they are linearly separable:\n\nx[y == 1, ] <- x[y == 1, ] + 0.5\nplot (x, col = (y + 5) / 2, pch = 16)\n\n\n\n\nNow the observations are just barely linearly separable. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified.\n\ndat <- data.frame(x = x, y = as.factor(y)) \nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1e5 )\n\nplot(svmfit , dat) \n\n\n\n\nNo training errors were made and only three support vectors were used. However, we can see from the figure that the margin is very narrow (because the observations that are not support vectors, indicated as circles, are very close to the decision boundary). It seems likely that this model will perform poorly on test data. We now try a smaller value of cost:\n\nsvmfit <- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1)\nsummary(svmfit) \n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  5\n\n ( 2 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\nplot(svmfit, dat) \n\n\n\n\nUsing cost = 1, we misclassify a training observation, but we also obtain a much wider margin and make use of seven support vectors. It seems likely that this model will perform better on test data than the model with cost = 1e5."
  },
  {
    "objectID": "lab09-svm.html#non-linear-svm",
    "href": "lab09-svm.html#non-linear-svm",
    "title": "Lab 09 SVM",
    "section": "2 Non-linear SVM",
    "text": "2 Non-linear SVM\n<<<<<<< Updated upstream In order to fit an SVM using a non-linear kernel, we once again use the svm() function. However, now we use a diﬀerent value of the parameter kernel. ======= In order to fit an SVM using a non-linear kernel, we once again use the svm() function. However, now we use a diﬀerent value of the parameter kernel. >>>>>>> Stashed changes\nTo fit an SVM with a polynomial kernel we use kernel = \"polynomial\", and to fit an SVM with a radial kernel we use kernel = \"radial\". In the former case we also use the degree argument to specify a degree for the polynomial kernel (the degree of “bendiness” in the separator), and in the latter case we use gamma to specify a value of \\(\\gamma\\) for the radial basis kernel. For details on these, see James et al. 2021 Ch 9.\nWe first generate some data with a non-linear class boundary, as follows:\n\nset.seed(1)\nx <- matrix(rnorm(200 * 2), ncol = 2)\nx[1:100, ] <- x [1:100, ] + 2\nx[ 101:150 , ] <- x [101:150, ] - 2\ny <- c(rep(1, 150),  rep(2, 50))\ndat <- data.frame(x = x, y = as.factor(y)) \n\n# always plot\nplot (x, col = y, pch = 16)\n\n\n\n\nThe data is randomly split into training and testing groups. We then fit the training data using the svm() function with a radial kernel and \\(\\gamma\\) = 1:\n\ntrain <- sample(200, 100)\nsvmfit <- svm(y ~ ., data = dat[train, ], \n              kernel = \"radial\", gamma = 1 ,cost = 1)\nplot(svmfit, dat[train, ]) # ok!\n\n\n\n\nThe plot shows that the resulting SVM has a decidedly non-linear boundary. The summary() function can be used to obtain some information about the SVM fit:\n\nsummary(svmfit) \n\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  31\n\n ( 16 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n\n\nWe can see from the figure that there are a fair number of training errors in this SVM fit. If we increase the value of cost, we can reduce the number of training errors. However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data.\n\nsvmfit <- svm(y ~ ., data = dat[train, ], kernel = \"radial\",\n              gamma = 1, cost = 1e5 )\nplot(svmfit, dat[train, ])\n\n\n\n\n\nTune it\n<<<<<<< Updated upstream We can perform cross-validation using tune() to select the best choice of \\(\\gamma\\) and cost for an SVM with a radial kernel: ======= We can perform cross-validation using tune() to select the best choice of \\(\\gamma\\) and cost for an SVM with a radial kernel: >>>>>>> Stashed changes\n\nset.seed(1)\ntune.out <- tune(svm, y ~ ., data = dat[train, ], kernel = \"radial\",\nranges = list (\n  cost = c (0.1, 1, 10, 100, 1000),\n  gamma = c (0.5, 1, 2, 3, 4)\n  )\n)\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.07 \n\n- Detailed performance results:\n    cost gamma error dispersion\n1  1e-01   0.5  0.26 0.15776213\n2  1e+00   0.5  0.07 0.08232726\n3  1e+01   0.5  0.07 0.08232726\n4  1e+02   0.5  0.14 0.15055453\n5  1e+03   0.5  0.11 0.07378648\n6  1e-01   1.0  0.22 0.16193277\n7  1e+00   1.0  0.07 0.08232726\n8  1e+01   1.0  0.09 0.07378648\n9  1e+02   1.0  0.12 0.12292726\n10 1e+03   1.0  0.11 0.11005049\n11 1e-01   2.0  0.27 0.15670212\n12 1e+00   2.0  0.07 0.08232726\n13 1e+01   2.0  0.11 0.07378648\n14 1e+02   2.0  0.12 0.13165612\n15 1e+03   2.0  0.16 0.13498971\n16 1e-01   3.0  0.27 0.15670212\n17 1e+00   3.0  0.07 0.08232726\n18 1e+01   3.0  0.08 0.07888106\n19 1e+02   3.0  0.13 0.14181365\n20 1e+03   3.0  0.15 0.13540064\n21 1e-01   4.0  0.27 0.15670212\n22 1e+00   4.0  0.07 0.08232726\n23 1e+01   4.0  0.09 0.07378648\n24 1e+02   4.0  0.13 0.14181365\n25 1e+03   4.0  0.15 0.13540064\n\n\nTherefore, the best choice of parameters involves cost = 1 and gamma = 0.5. We can view the test set predictions for this model by applying the predict() function to the data. Notice that to do this we subset the dataframe dat using -train as an index set.\n\ntable(\n  true = dat[-train, \"y\"], pred = predict (\n    tune.out$best.model, newdata = dat[-train, ]\n    )\n  )\n\n    pred\ntrue  1  2\n   1 67 10\n   2  2 21\n\n\nOnly 12% of test observations are misclassified by this SVM."
  },
  {
    "objectID": "lab09-svm.html#roc-curves",
    "href": "lab09-svm.html#roc-curves",
    "title": "Lab 09 SVM",
    "section": "3 ROC curves",
    "text": "3 ROC curves\nThe {ROCR} package can be used to produce ROC curves (see ch 9 in James et al. 2021). We first write a short function to plot an ROC curve given a vector containing a numerical score for each observation, pred, and a vector containing the class label for each observation, truth.\n\nlibrary(ROCR)\n\nrocplot <- function(pred, truth , ...) {\n  predob <- prediction (pred , truth )\n  perf <- performance (predob , \" tpr \", \" fpr \")\n  plot (perf , ...)\n  }\n\n<<<<<<< Updated upstream SVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation \\(X = (X_1, X_2,. .., X_p)^T\\) takes the form \\(\\hat\\beta_0 + \\hat\\beta_1X_1 + \\hat\\beta_2X_2 + + \\hat\\beta_pX_p\\). ======= SVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation \\(X = (X_1, X_2,. .., X_p)^T\\) takes the form \\(\\hat\\beta_0 + \\hat\\beta_1X_1 + \\hat\\beta_2X_2 + + \\hat\\beta_pX_p\\). >>>>>>> Stashed changes\nIn essence, the sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. In order to obtain the fitted values for a given SVM model fit, we use decision.values = TRUE when fitting svm(). Then the predict() function will output the fitted values.\n\nsvmfit.opt <- svm(y ~ ., data = dat[train, ], \nkernel = \"radial\", gamma = 2, cost = 1, decision.values = T)\n\nfitted <- attributes(\n  predict(svmfit.opt, dat[ train, ], decision.values = TRUE)\n  )$decision.values"
  },
  {
    "objectID": "lab09-svm.html#exercises",
    "href": "lab09-svm.html#exercises",
    "title": "Lab 09 SVM",
    "section": "6 Exercises",
    "text": "6 Exercises\n\nExercise 1"
  },
  {
    "objectID": "lab09-svm.html#resources",
    "href": "lab09-svm.html#resources",
    "title": "Lab 09 SVM",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "lab09-svm.html#harper-adams-data-science",
    "href": "lab09-svm.html#harper-adams-data-science",
    "title": "Lab 09 SVM",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab10-unsupervised.html#exercises",
    "href": "lab10-unsupervised.html#exercises",
    "title": "Lab 10 Unsupervised",
    "section": "6 Exercises",
    "text": "6 Exercises\n\nExercise 1"
  },
  {
    "objectID": "lab10-unsupervised.html#resources",
    "href": "lab10-unsupervised.html#resources",
    "title": "Lab 10 Unsupervised",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "lab10-unsupervised.html#harper-adams-data-science",
    "href": "lab10-unsupervised.html#harper-adams-data-science",
    "title": "Lab 10 Unsupervised",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lec00-module-intro.html#section",
    "href": "lec00-module-intro.html#section",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "",
    "text": "C7081 Statistical Analysis for Data Science\nEd Harris"
  },
  {
    "objectID": "lec00-module-intro.html#module-overview",
    "href": "lec00-module-intro.html#module-overview",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "Module overview",
    "text": "Module overview\n\n\n\n\n“I am always ready to learn although I do not always like being taught.”\n\n-Winston Churchill"
  },
  {
    "objectID": "lec00-module-intro.html#module-overview-1",
    "href": "lec00-module-intro.html#module-overview-1",
    "title": "C7081-2022 Statistical analysis for data science",
    "section": "Module overview",
    "text": "Module overview\n\n\nOne week intensive format\nDaily catch-up meetings\nRecorded lectures, readings\nLabs are critical"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase data science skills (e.g. a github project website that could be part of your cv). For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\n\n\n\nDay\nTopics\nLabs\nReadings\n\n\n\n\nInduction\n welcome activity \nLab welcome exercise\n\n\n\nMon\nlive:\n am\n pm\n*lecture videos password: data4life\n 00 Module overview\n 01 Introduction 1.1 1.2\n 02 Statistical learning 2.1 2.2\nLab guidance\nLab 01 Linear algebra fun\nLab 02 R programming refresh\nJames et al. 2021 Ch 1,2\nEfron 2020\n\n\nTues\nam\npm\n 03 Linear regression 3.1 3.2\n 04 Classification 4.1 4.2\nLab 03 Linear regression\nLab 04 Classification\nJames et al. 2021 Ch 3,4\nMelesse 2018\n\n\nWed\nam\npm\n 05 Bootstrapping 5.1 5.2\n 06 Model selection 6.1 6.2 6.3\nLab 05 Resampling\nLab 06 Model selection\nJames et al. 2021 Ch 5,6\nAho 2014\n\n\nThurs\nam\npm\n 07 Non-linear models 7.1 7.2\n 08 Decision trees 8.1 8.2 8.3 8.4\nLab 07 Non-linear models\nLab 08 Decision trees\nJames et al. 2021 Ch 7,8\nBarnard 2019\nOtukei 2010\n\n\nFri\nam\npm\n 09 Support vector machines 9.1 9.2\n 10 Unsupervised learning 10.1 10.2 10.3\nLab 09 SVM\nLab 10 Unsupervised learning\nJames et al. 2021 Ch 9,12\nEbrahimi 2017\nHowell 2020"
  },
  {
    "objectID": "schedule.html#references",
    "href": "schedule.html#references",
    "title": "Schedule",
    "section": "References",
    "text": "References\nTextbook: James et al. 2021 Introduction to statistical learning 2ed\nall refs zip\nAho, K., Derryberry, D., Peterson, T., 2014. Model selection for ecologists: the worldviews of AIC and BIC. Ecology 95, 631–636.\nBarnard, D.M., Germino, M.J., Pilliod, D.S., Arkle, R.S., Applestein, C., Davidson, B.E., Fisk, M.R., 2019. Cannot see the random forest for the decision trees: selecting predictive models for restoration ecology. Restoration Ecology 27, 1053–1063.\nEbrahimi, M.A., Khoshtaghaza, M.H., Minaei, S., Jamshidi, B., 2017. Vision-based pest detection based on SVM classification method. Computers and Electronics in Agriculture 137, 52–58.\nEfron, B., 2020. Prediction, Estimation, and Attribution. Journal of the American Statistical Association 115, 636–655.\nHowell, O., Wenping, C., Marsland, R., Mehta, P., 2020. Machine learning as ecology. J. Phys. A: Math. Theor. 53, 334001.\nJames, G., Witten, D., Hastie, T., Tibshirani, R., 2021. An Introduction to Statistical Learning: with Applications in R, Springer Texts in Statistics 2ed. Springer-Verlag, New York.\nMelesse, S., Sobratee, N., Workneh, T., 2016. Application of logistic regression statistical technique to evaluate tomato quality subjected to different pre- and post-harvest treatments. Biological Agriculture & Horticulture 32, 277–287.\nOtukei, J.R., Blaschke, T., 2010. Land cover change assessment using decision trees, support vector machines and maximum likelihood classification algorithms. International Journal of Applied Earth Observation and Geoinformation, Supplement Issue on “Remote Sensing for Africa – A Special Collection from the African Association for Remote Sensing of the Environment (AARSE)” 12, S27–S31."
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  }
]